## Persönliche Hinweise und Erfahrungen

Dieses Projekt entstand vor allem aus meinem persönlichen Interesse und Experimentierdrang. Ich wollte herausfinden, ob es möglich ist, mit Hilfe von LLMs (Large Language Models) komplexeren Code zu generieren. Dabei war mir folgendes wichtig:

- **Für den eigenen Gebrauch:**  
  Das Projekt ist in erster Linie für mich entstanden. Falls es aber jemandem nützlich ist, freut es mich sehr – der Nutzen anderer ist ein willkommener Nebeneffekt meiner Experimente.

- **Verwendung auf eigene Verantwortung:**  
  Da der Code experimentell und als Grundlage für weitere Entwicklungen gedacht ist, erfolgt die Nutzung auf eigene Gefahr. Es gibt keine Garantie dafür, dass er in jeder Umgebung fehlerfrei läuft oder optimal funktioniert.

- **Optimierungen und Verbesserungen:**  
  Optimierungen, Erweiterungen oder Verbesserungen sind jederzeit willkommen. Das Projekt soll als offener Ausgangspunkt dienen, der nach Bedarf angepasst und optimiert werden kann.

- **Erkundung der Möglichkeiten von LLMs:**  
  Ein wesentlicher Antrieb war die Frage, ob LLMs dazu in der Lage sind, auch komplexeren Code zu generieren. Die Idee war, ihre Grenzen und Möglichkeiten praktisch zu erproben.

- **Erfahrungen mit verschiedenen Modellen:**  
  - **Free-Modelle wie "Le Chat":**  
    Diese Modelle liefern oft sehr allgemeine Ergebnisse. Sie neigen dazu, zu pauschale und wenig spezifische Antworten zu generieren, was bei komplexeren Aufgabenstellungen oft nicht ausreicht.
  
  - **Deepseek:**  
    Deepseek liefert teilweise zufriedenstellende Resultate. Allerdings ist der Dienst häufig nicht erreichbar, vor allem an Wochenenden, wenn viele Nutzer gleichzeitig darauf zugreifen.
  
  - **ChatGPT:**  
    ChatGPT liefert in vielen Fällen gute Ergebnisse, hat aber Einschränkungen beim reasoning (Schlussfolgerungsvermögen) und unterstützt das Hochladen von Dateien nur einmal pro Tag.
  
  - **Lokale Modelle:**  
    Lokale LLM-Implementierungen arbeiten oft sehr zäh und liefern Ergebnisse, die durch den geringeren Umfang der zugrunde liegenden Datenmodelle limitiert sind.

Diese Erfahrungen und Überlegungen flossen in die Entwicklung dieses Plugin-Grundgerüsts ein. 
Ziel war es, ein nützliches Experiment zu schaffen, das zeigt, wie moderne LLMs in der Lage sind,
 strukturierteren und komplexeren Code zu generieren – und dabei auch ihre Schwächen aufzeigt.
